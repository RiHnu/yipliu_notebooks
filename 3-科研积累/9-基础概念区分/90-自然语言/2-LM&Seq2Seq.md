# Language Model V.S Seq2Seq Model
[blog](https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8)

**Seq2Seq Model**
给出一个 Sequence, 得到另一个 Sequence https://albertauyeung.github.io/2020/06/19/bert-tokenization.html