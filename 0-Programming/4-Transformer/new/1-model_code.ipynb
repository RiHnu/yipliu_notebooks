{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "type(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Model 介绍\n",
    "\n",
    "## 0.1 BertModel\n",
    "\n",
    "> 组成\n",
    "\n",
    "- self.embeddings = BertEmbeddings\n",
    "- self.encoder   = BertEncoder\n",
    "- self.pooler    = BertPooler\n",
    "\n",
    "> 流程\n",
    "\n",
    "*Step1*\n",
    "\n",
    "input_ids ==> ***self.embeddings*** = embedding_output \n",
    "\n",
    "*Step2*\n",
    "\n",
    "embedding_output ==> ***self.encoder*** = encoder_outputs \n",
    "\n",
    "*Step3*\n",
    "encoder_outputs[0] (last_hidden_state) ==> ***self.pooler*** = pooled_output\n",
    "\n",
    "\n",
    "> 总结\n",
    "\n",
    "1. last_hidden_state: Model 最后一层的输出 [B, S, H]\n",
    "\n",
    "2. pooler_output: 最后一层 [CLS] 对应的特征 送入 self.pooler 后得到的向量 [B, H]\n",
    "\n",
    "3. hidden_states: output_hidden_states=True. Model 每层对应的输出 [B, S, H]\n",
    "                  第一层是 Embedding layer 对应的值: Embedding layer 由 word_embeddings, position_embeddings等组合而成\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.1 BertEmbeddings\n",
    "\n",
    ">组成\n",
    "\n",
    "- word_embeddings\n",
    "- position_embeddings\n",
    "- token_type_embeddings\n",
    "\n",
    "> 流程: input_ids\n",
    "\n",
    "**输入**: input_ids \n",
    "\n",
    "*Step1*\n",
    "\n",
    "input_ids ==> ***self.word_embeddings*** = inputs_embeds\n",
    "\n",
    "*Step2*\n",
    "\n",
    "embeddings = inputs_embeds + token_type_embeddings\n",
    "\n",
    "*Step3*\n",
    "\n",
    "embeddings += position_embeddings 并进行 LN 等操作\n",
    "\n",
    "最终\n",
    "\n",
    "**输出** embeddings\n",
    "\n",
    "\n",
    "> 流程: inputs_embeds\n",
    "\n",
    "当输入为 inputs_embeds 时，Model 认为 它是 word_embeddings 对应的值.\n",
    "\n",
    "它需要接着去经历 token_type_embeddings, position_embeddings, LN 和 dropout 操作, 才能得到最终 Embedding Layer 对应的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2 BertEncoder\n",
    "\n",
    "> 组成\n",
    "\n",
    "self.layer = 多层的 BertLayer\n",
    "\n",
    "> 流程\n",
    "\n",
    "- **输入**\n",
    "\n",
    "hidden_states (embedding_output) ==> **self.layer** = all_hidden_states\n",
    "\n",
    "- **输出**\n",
    "\n",
    "last_hidden_state\n",
    "\n",
    "hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24e149ca21b7977f113f7cae485a610638989eae958e24203fa60c141501a44e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pylightnlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
