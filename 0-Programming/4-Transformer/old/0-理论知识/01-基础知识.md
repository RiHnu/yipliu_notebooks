# BERT

[优秀BLOG](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/)

[BERT Embedding](https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)

[BERT 的数据处理](https://albertauyeung.github.io/2020/06/19/bert-tokenization.html)

## 1. Motivation
作者认为现有技术（关于利用预训练 LM 的表征，处理下游任务）限制了 LM 的表征能力

现有 LM 模型是单向的，这限制了在预训练期间可以使用的架构

GPT 使用的是 left-to-right 架构，这导致每个单词仅仅只能对其 previous tokens 进行 **Attention**
> 作者认为，这种单向的结构在 Sentence-level tasks 中是次优的，例如在 QA 问题中，一个 token 的context 来自两个方向

## 2. CLS token 知识点


## BERT 的训练
训练任务：以 15% 的概率对 Tokens 进行 Masked, 
1. Masked Language Modeling
> 猜出被 mask 的 Token

2. Next Sentence Prediction (NSP)
> 给出两个句子，预测第二个句子是否是第一个句子的 Next Sentence (binary classification)