# src_mask 和 src_key_padding_mask

src_mask 是 attn_mask [S, S]
> 输入 Sequence  的哪部分需要被 attention


src_key_padding_mask 是 key_padding_mask [B, S]
> 标记输入中的 padding
我们要用的参数是 src_key_padding_mask


输入:
`src`

`mask`
> 主要用在 decoder 中. shape=[S, S]

`src_key_padding_mask`: 
>mask out positions of input sequence that are padding

- 若为 bool, 代码会自动转化为 float

剔除 输入序列中 PAD

`is_causal`



注意事项:
1. `mask` 和 `src_key_padding_mask`  dtype 要相同 bool or  float



# 代码逻辑

`STEP 1`
由 src 得到 seq_len

`is_causal ` True or False. Default=None

>当 给了 mask, 就要考虑 `is_causal` 情况

**情况**

```pytorch
mask = tensor.zeros(s, s)

# 将 mask 传入到 Transformer 中

# 代码内部得到 mask_causal: 下三角为 True, 上三角为 False
```

output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)


在nn.Transformer* `is_causal` 只是一个 hint, 表明 mask 是 causal  

正常情况下 不用这个参数