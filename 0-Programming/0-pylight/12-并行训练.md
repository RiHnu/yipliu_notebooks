# Model Parallel GPU training
[官网操作](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/accelerator_prepare.html)


第一种：
例如 把 nn.Embedding Layer 切分两份，分别放在 GPU_0 和 GPU_1 上

第二种：
把 不同层放在不同的 GPU 上 



Sharded Training


当使用并行训练时候，我们需要使用

validation_step_end 和 training_step_end



# 操作方法

1. 修改 Trainer 参数

# tensor.to(tensorB)

`torch.dtype` 和 `torch.device` 都继承 argument tensorB


# 移动到相同device的方法
[issue](https://github.com/Lightning-AI/lightning/issues/14554)